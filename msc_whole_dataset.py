# -*- coding: utf-8 -*-
"""MSc_Whole_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17y7RqFnvulp3dTRaSsbR5oCgLR8NW2ov
"""

from google.colab import drive
drive.mount('/content/drive/')

"""## 1 Installs all required packages

The code I've provided: Installs all required packages (ultralytics for YOLO, OpenCV, pandas, scikit-learn, etc.) Imports the necessary libraries Sets up your file paths according to your Colab structure Checks for GPU availability (important for YOLO performance) Defines the 7 types of Lean waste (Muda) that we'll be detecting Please run this code in your Colab notebook. Once you've confirmed it runs successfully, we'll move on to the next step: Video and Data Preprocessing. For your thesis, this structured approach ensures reproducibility and clear documentation of your methodology. Each step builds on the previous one, making it easier to troubleshoot and optimize specific components of your pipeline.
"""

# Install required packages
!pip install ultralytics  # For YOLO
!pip install opencv-python-headless  # For video processing
!pip install pandas matplotlib seaborn  # For data handling and visualization
!pip install scikit-learn  # For ML metrics and evaluation
!pip install openai  # For LLM integration
!pip install plotly dash  # For interactive dashboard
!pip install ipywidgets  # For interactive components in Colab
!pip install tqdm  # For progress bars

import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
from ultralytics import YOLO
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import torch
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

import os

# === Base directory for the whole dataset ===
BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset'

# === Folder paths for batch processing ===
VIDEO_DIR = os.path.join(BASE_PATH, 'videos')              # Folder with all .mp4 videos
ANNOTATION_DIR = os.path.join(BASE_PATH, 'annotations')    # Folder with matching .csv files
STEPS_IDS_PATH = os.path.join(BASE_PATH, 'steps_ids.csv')  # Step ID mapping file
WEIGHTS_PATH = os.path.join(BASE_PATH, 'weights.pt')       # YOLO model weights
OUTPUT_DIR = os.path.join(BASE_PATH, 'results')            # Output folder for saving results

# === Create output directory if it doesn't exist ===
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === (Optional) Confirm directory contents ===
print("ðŸ“‚ VIDEO_DIR:", VIDEO_DIR)
print("ðŸ“‚ ANNOTATION_DIR:", ANNOTATION_DIR)
print("ðŸ“ OUTPUT_DIR:", OUTPUT_DIR)
print("ðŸ“„ STEPS_IDS_PATH:", STEPS_IDS_PATH)
print("ðŸ“„ WEIGHTS_PATH:", WEIGHTS_PATH)

# Check if GPU is available
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# Define the 7 types of Lean waste (Muda)
MUDA_TYPES = {
    0: "No waste",
    1: "Transportation",  # Unnecessary movement of materials
    2: "Inventory",       # Excess storage and delay
    3: "Motion",          # Unnecessary movement of people
    4: "Waiting",         # Idle time
    5: "Overproduction",  # Making more than needed
    6: "Overprocessing",  # More work or higher quality than needed
    7: "Defects"          # Errors requiring rework
}

print("Setup complete!")

"""## Step 2: Video and Data Preprocessing

This code handles: Video Loading and Analysis: Extracts key information like frame count, FPS, resolution, and duration CSV Data Processing: Loads and merges your step information from both CSV files Frame Extraction: Systematically extracts frames at regular intervals (every 30th frame by default) Step Mapping: Maps each extracted frame to its corresponding process step based on timestamps Visualization: Creates a timeline visualization of your woodworking process steps The preprocessing stage is crucial for your thesis as it structures your data for analysis and establishes the connection between video frames and process steps. This foundation will enable the subsequent waste detection components to properly contextualize their findings. Please run this code in your Colab notebook. You can adjust the frame_interval parameter based on your needs - a smaller value will give you more frames for analysis but will increase processing time. Once you've confirmed this runs successfully, we'll move on to Step 3: Object Detection using your custom YOLO weights.
"""

import os
import cv2
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

# === Define functions ===

def load_video_info(video_path):
    cap = cv2.VideoCapture(video_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    duration = frame_count / fps if fps > 0 else 0
    cap.release()
    return {
        'path': video_path,
        'frame_count': frame_count,
        'fps': fps,
        'width': width,
        'height': height,
        'duration': duration
    }

def load_csv_data(csv_path, steps_ids_path):
    video_df = pd.read_csv(csv_path, header=None)
    video_df.columns = ['step_id', 'start_time', 'end_time']
    steps_df = pd.read_csv(steps_ids_path, header=None)
    steps_df.columns = ['step_id', 'activity']
    merged_df = pd.merge(video_df, steps_df, on='step_id', how='inner')
    return merged_df

def extract_frames(video_path, output_dir, interval=30, start_time_sec=0.0):
    os.makedirs(output_dir, exist_ok=True)
    cap = cv2.VideoCapture(video_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_to_time = {frame_num: frame_num / fps for frame_num in range(frame_count)}
    start_frame = int(start_time_sec * fps)
    frames_data = []
    for frame_num in range(start_frame, frame_count, interval):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
        ret, frame = cap.read()
        if ret:
            frame_path = os.path.join(output_dir, f"frame_{frame_num:06d}.jpg")
            cv2.imwrite(frame_path, frame)
            frames_data.append({
                'frame_num': frame_num,
                'timestamp': frame_to_time[frame_num],
                'path': frame_path
            })
    cap.release()
    return pd.DataFrame(frames_data)

def map_frames_to_steps(frames_df, steps_df):
    frames_df['step_id'] = None
    frames_df['activity'] = None
    for _, row in steps_df.iterrows():
        step_id = row['step_id']
        activity = row['activity']
        start_time = row['start_time']
        end_time = row['end_time']
        mask = (frames_df['timestamp'] >= start_time) & (frames_df['timestamp'] <= end_time)
        frames_df.loc[mask, 'step_id'] = step_id
        frames_df.loc[mask, 'activity'] = activity
    return frames_df

# === Main batch preprocessing function ===

def preprocess_all_videos(base_path, video_dir, annotation_dir, steps_ids_path, output_dir, frame_interval=30):
    video_files = sorted([f for f in os.listdir(video_dir) if f.endswith('.mp4')])
    steps_ids_df = pd.read_csv(steps_ids_path, header=None)
    steps_ids_df.columns = ['step_id', 'activity']

    for video_file in tqdm(video_files, desc="Processing videos"):
        video_id = video_file.replace('.mp4', '')
        video_prefix = '_'.join(video_id.split('_')[:2])  # e.g. 01_glove
        annotation_path = os.path.join(annotation_dir, f"{video_prefix}.csv")
        video_path = os.path.join(video_dir, video_file)
        video_output_path = os.path.join(output_dir, video_id)
        frames_output_path = os.path.join(video_output_path, 'frames')

        if not os.path.exists(annotation_path):
            print(f"âŒ Skipping {video_id}: annotation file '{video_prefix}.csv' not found.")
            continue

        os.makedirs(video_output_path, exist_ok=True)

        # Load metadata
        video_info = load_video_info(video_path)
        steps_df = load_csv_data(annotation_path, steps_ids_path)
        min_step_time = steps_df['start_time'].min()

        # Frame extraction + mapping
        frames_df = extract_frames(video_path, frames_output_path, interval=frame_interval, start_time_sec=min_step_time)
        frames_df = map_frames_to_steps(frames_df, steps_df)

        # Save processed data
        frames_df.to_csv(os.path.join(video_output_path, 'preprocessed_frames.csv'), index=False)
        steps_df.to_csv(os.path.join(video_output_path, 'preprocessed_steps.csv'), index=False)

        # Create step timeline plot
        plt.figure(figsize=(12, 6))
        for idx, row in steps_df.iterrows():
            plt.barh(row['activity'],
                     width=row['end_time'] - row['start_time'],
                     left=row['start_time'],
                     height=0.5,
                     color=plt.cm.viridis(idx / len(steps_df)))
        plt.xlabel('Time (seconds)')
        plt.ylabel('Process Step')
        plt.title(f'Process Timeline â€“ {video_id}')
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig(os.path.join(video_output_path, 'process_timeline.png'))
        plt.close()

    print("âœ… Batch preprocessing complete!")

# === Call the function with dataset paths ===

BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset'
VIDEO_DIR = os.path.join(BASE_PATH, 'videos')
ANNOTATION_DIR = os.path.join(BASE_PATH, 'annotations')
STEPS_IDS_PATH = os.path.join(BASE_PATH, 'steps_ids.csv')
OUTPUT_DIR = os.path.join(BASE_PATH, 'results')

preprocess_all_videos(BASE_PATH, VIDEO_DIR, ANNOTATION_DIR, STEPS_IDS_PATH, OUTPUT_DIR, frame_interval=30)

"""## 3. Object Detection with YOLO Weights

This code implements: YOLO Model Loading: Loads your custom YOLO model using the weights.pt file you provided Object Detection: Processes each extracted frame to identify and locate objects Result Storage: Saves annotated images and creates a comprehensive DataFrame with detection details Detection Analysis: Generates statistics and visualizations including: Object counts by class Objects detected per process step Confidence score distributions Timeline of object appearances throughout the video For your thesis, this object detection component is crucial as it identifies the tools, materials, and other elements in each frame. These detections will serve as the foundation for identifying inefficiencies in the process. The confidence threshold (default 0.25) can be adjusted based on your needs - higher for precision, lower for recall. Once you've run this code successfully in your Colab notebook, we'll move on to Step 4: Motion Detection, which will build on these object detections to analyze movement patterns.
"""

# Import additional libraries for object detection
import torch
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
import cv2 # Ensure cv2 is imported here if needed
import pandas as pd # Ensure pandas is imported here if needed
import os # Ensure os is imported here if needed
from tqdm.notebook import tqdm # Ensure tqdm is imported here if needed
from ultralytics import YOLO # Ensure YOLO is imported here

# === Load YOLO model ===
def load_yolo_model(weights_path):
    if not os.path.exists(weights_path):
        raise FileNotFoundError(f"Weights file not found: {weights_path}")
    model = YOLO(weights_path)
    print(f"âœ… YOLO model loaded from {weights_path}")
    return model

def detect_objects_batched(model, frames_df, output_dir, batch_size=16, conf_threshold=0.25, iou_threshold=0.45):
    os.makedirs(output_dir, exist_ok=True)
    all_detections = []
    frames_list = frames_df.to_dict(orient='records')

    for i in tqdm(range(0, len(frames_list), batch_size), desc="Detecting objects (batched)"):
        batch = frames_list[i:i+batch_size]
        paths = [f['path'] for f in batch if os.path.exists(f['path'])]
        if not paths: continue

        results = model.predict(source=paths, conf=conf_threshold, iou=iou_threshold, verbose=False)

        for frame_info, result in zip(batch, results):
            frame_num = frame_info['frame_num']
            timestamp = frame_info['timestamp']
            step_id = frame_info.get('step_id')
            activity = frame_info.get('activity')
            frame_path = frame_info['path']

            if result.boxes is not None and len(result.boxes) > 0:
                annotated_img_path = os.path.join(output_dir, f"detected_frame_{frame_num:06d}.jpg")
                img = result.plot()
                cv2.imwrite(annotated_img_path, img)

                for box in result.boxes:
                    det = box.data[0]
                    class_id = int(det[5])
                    confidence = float(det[4])
                    x1, y1, x2, y2 = det[:4].tolist()

                    all_detections.append({
                        'frame_num': frame_num,
                        'timestamp': timestamp,
                        'step_id': step_id,
                        'activity': activity,
                        'class_id': class_id,
                        'class_name': model.names[class_id],
                        'confidence': confidence,
                        'x1': x1,
                        'y1': y1,
                        'x2': x2,
                        'y2': y2,
                        'annotated_path': annotated_img_path
                    })

    detections_df = pd.DataFrame(all_detections)
    detections_csv_path = os.path.join(output_dir, 'object_detections.csv')
    detections_df.to_csv(detections_csv_path, index=False)
    print(f"ðŸ’¾ Saved {len(detections_df)} detections to {detections_csv_path}")
    return detections_df

def run_object_detection_on_dataset(base_path, batch_size=16):
    results_dir = os.path.join(base_path, 'results')
    weights_path = os.path.join(base_path, 'weights.pt')
    model = load_yolo_model(weights_path)
    video_folders = sorted([f for f in os.listdir(results_dir) if os.path.isdir(os.path.join(results_dir, f))])

    for video_id in tqdm(video_folders, desc="Running detection on all videos"):
        video_folder = os.path.join(results_dir, video_id)
        frames_csv = os.path.join(video_folder, 'preprocessed_frames.csv')
        if not os.path.exists(frames_csv):
            print(f"âš ï¸ Skipping {video_id} â€” frames CSV not found.")
            continue

        frames_df = pd.read_csv(frames_csv)
        detection_output_dir = os.path.join(video_folder, 'object_detection')
        print(f"ðŸ” Detecting objects in {video_id}...")
        detect_objects_batched(model, frames_df, detection_output_dir, batch_size=batch_size)

    print("âœ… Batch object detection complete.")

# Path to your dataset
BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset'
run_object_detection_on_dataset(BASE_PATH, batch_size=16)

"""## 3.2 Objetc Tracking"""

model = YOLO("/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/weights.pt")
print(model.names)

# âš ï¸ Install this in a new Colab cell before running:
# !pip install ultralytics opencv-python-headless pandas tqdm

import os
import cv2
import pandas as pd
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from ultralytics import YOLO

# === Tracking function for a single video ===
def run_yolov8_tracking(frames_df, weights_path, output_dir, conf=0.25, iou=0.45, tracker_cfg="botsort.yaml"):
    os.makedirs(output_dir, exist_ok=True)

    # Load YOLO model
    model = YOLO(weights_path)
    print(f"âœ… Loaded model from: {weights_path}")
    print(f"Classes: {model.names}")

    all_tracks = []

    for _, row in tqdm(frames_df.iterrows(), total=len(frames_df), desc="ðŸ”„ Tracking objects"):
        frame_path = row['path']
        frame_num = row['frame_num']
        timestamp = row['timestamp']
        step_id = row['step_id']
        activity = row['activity']

        if not os.path.exists(frame_path):
            continue

        results = model.track(
            source=frame_path,
            persist=True,
            conf=conf,
            iou=iou,
            tracker=tracker_cfg,
            verbose=False
        )

        if not results or not results[0].boxes:
            continue

        boxes = results[0].boxes

        for box in boxes:
            b = box.data[0].cpu().numpy()
            x1, y1, x2, y2 = b[0:4]
            conf_score = b[4]

            # âœ… FIX: Extract class ID properly
            if hasattr(box, 'cls') and box.cls is not None:
                cls_id = int(box.cls.cpu().numpy()[0])
            else:
                cls_id = int(b[5]) if len(b) > 5 else 0

            # Get track ID
            track_id = int(box.id.cpu().numpy()[0]) if hasattr(box, 'id') and box.id is not None else -1

            # Map class ID to name
            class_name = model.names.get(cls_id, f"Class_{cls_id}")

            all_tracks.append({
                'frame_num': frame_num,
                'timestamp': timestamp,
                'step_id': step_id,
                'activity': activity,
                'class_id': cls_id,
                'class_name': class_name,
                'confidence': conf_score,
                'x1': x1,
                'y1': y1,
                'x2': x2,
                'y2': y2,
                'width': x2 - x1,
                'height': y2 - y1,
                'area': (x2 - x1) * (y2 - y1),
                'track_id': track_id,
                'frame_path': frame_path
            })

    tracks_df = pd.DataFrame(all_tracks)
    save_path = os.path.join(output_dir, 'tracked_objects.csv')
    tracks_df.to_csv(save_path, index=False)
    print(f"ðŸ’¾ Saved tracked objects to: {save_path} ({len(tracks_df)} entries)")
    return tracks_df

# === Batch run tracking across all videos in dataset ===
def batch_run_yolov8_tracking(base_path, tracker_cfg="botsort.yaml", max_workers=2):
    results_dir = os.path.join(base_path, 'results')
    weights_path = os.path.join(base_path, 'weights.pt')

    video_folders = sorted([f for f in os.listdir(results_dir) if os.path.isdir(os.path.join(results_dir, f))])
    total = len(video_folders)
    print(f"ðŸš€ Starting tracking on {total} videos (parallel: {max_workers})")

    def process_single_tracking(video_id):
        try:
            video_folder = os.path.join(results_dir, video_id)
            frames_csv = os.path.join(video_folder, 'preprocessed_frames.csv')
            if not os.path.exists(frames_csv):
                return f"âš ï¸ Skipped {video_id}: frames CSV not found."

            frames_df = pd.read_csv(frames_csv)
            tracking_output_dir = os.path.join(video_folder, 'object_tracking')
            run_yolov8_tracking(frames_df, weights_path, tracking_output_dir, tracker_cfg=tracker_cfg)
            return f"âœ… Tracked {video_id}"
        except Exception as e:
            return f"âŒ Error tracking {video_id}: {e}"

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_single_tracking, vid): vid for vid in video_folders}
        for idx, future in enumerate(as_completed(futures)):
            print(f"[{idx+1}/{total}] {future.result()} ({(idx+1)/total:.0%})")

    print("âœ… All object tracking complete.")

# === Manual entry point ===
BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset'
batch_run_yolov8_tracking(BASE_PATH, tracker_cfg="botsort.yaml", max_workers=1)

"""## 4. Motion Detection with YOLO

This code implements: Object Tracking: Tracks objects across video frames using YOLO's tracking capabilities Motion Metrics Calculation: Computes key metrics for each tracked object including: Total distance traveled Average and maximum speed Duration of appearance Changes in object size/area Motion Pattern Analysis: Analyzes movement patterns by activity type and object class Visualization Generation: Creates insightful visualizations including: Motion heatmap showing areas of high activity Object trajectories showing movement paths Comparative charts for speed and distance by activity and object class For your master thesis, this motion detection component is essential for identifying inefficiencies related to unnecessary movement (Motion waste) and waiting (Waiting waste). The metrics and visualizations provide quantitative evidence of potential waste in the woodworking process. Once you've run this code successfully in your Colab notebook, we'll move on to Step 5: Waste Detection - Muda Classifier, which will use the object and motion data to identify specific types of Lean waste.
"""

import os
import cv2
import pandas as pd
import numpy as np
import sys
from tqdm import tqdm
from ultralytics import YOLO
import matplotlib.pyplot as plt
import seaborn as sns
import torch

# -------------------- UTILITY FUNCTIONS --------------------

def track_poses(pose_model, frames_df, output_dir, video_id, conf_threshold=0.25):
    os.makedirs(output_dir, exist_ok=True)
    frames_df = frames_df.sort_values('frame_num')
    all_tracks = []

    total_frames = len(frames_df)
    for idx, (_, row) in enumerate(frames_df.iterrows(), start=1):
        frame_path = row['path']
        if not os.path.exists(frame_path):
            continue

        result = pose_model.track(frame_path, conf=conf_threshold, persist=True, verbose=False)[0]
        annotated = result.plot()
        annotated_path = os.path.join(output_dir, f"pose_{row['frame_num']:06d}.jpg")
        cv2.imwrite(annotated_path, annotated)

        if hasattr(result, 'keypoints') and result.keypoints is not None:
            kpts = result.keypoints.data.cpu().numpy()
            ids = result.boxes.id.cpu().numpy().astype(int) if result.boxes.id is not None else range(len(kpts))

            for keypoint_array, track_id in zip(kpts, ids):
                valid = keypoint_array[keypoint_array[:, 2] > 0.5]
                if len(valid) == 0:
                    continue
                cx, cy = valid[:, 0].mean(), valid[:, 1].mean()
                x1, y1, x2, y2 = valid[:, 0].min(), valid[:, 1].min(), valid[:, 0].max(), valid[:, 1].max()

                row_data = {
                    'frame_num': row['frame_num'],
                    'timestamp': row['timestamp'],
                    'step_id': row['step_id'],
                    'activity': row['activity'],
                    'track_id': int(track_id),
                    'center_x': cx,
                    'center_y': cy,
                    'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,
                    'width': x2 - x1,
                    'height': y2 - y1,
                    'area': (x2 - x1) * (y2 - y1),
                    'annotated_path': annotated_path
                }

                key_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
                             'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
                             'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
                             'left_knee', 'right_knee', 'left_ankle', 'right_ankle']

                for i, name in enumerate(key_names):
                    if i < len(keypoint_array) and keypoint_array[i, 2] > 0.5:
                        row_data[f'{name}_x'] = keypoint_array[i, 0]
                        row_data[f'{name}_y'] = keypoint_array[i, 1]
                        row_data[f'{name}_conf'] = keypoint_array[i, 2]

                all_tracks.append(row_data)

        percent = int((idx / total_frames) * 100)
        sys.stdout.write(f"\r[{video_id}] â³ {percent}% completed")
        sys.stdout.flush()

    print(f"\r[{video_id}] âœ… 100% completed â€” {len(all_tracks)} poses tracked.")

    df = pd.DataFrame(all_tracks)
    if not df.empty:
        df.to_csv(os.path.join(output_dir, 'pose_tracks.csv'), index=False)
    return df

def calculate_pose_motion_metrics(tracks_df, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    if tracks_df.empty or 'track_id' not in tracks_df:
        return pd.DataFrame()

    metrics = []
    grouped = tracks_df[tracks_df.track_id >= 0].groupby(['track_id', 'step_id'])

    for (tid, sid), g in tqdm(grouped, desc="Calculating motion metrics"):
        g = g.sort_values('frame_num')
        if len(g) < 2:
            continue
        dists = [np.linalg.norm([g.iloc[i+1]['center_x'] - g.iloc[i]['center_x'],
                                 g.iloc[i+1]['center_y'] - g.iloc[i]['center_y']])
                 for i in range(len(g) - 1)]
        times = [g.iloc[i+1]['timestamp'] - g.iloc[i]['timestamp'] for i in range(len(g) - 1)]
        speeds = [d/t if t > 0 else 0 for d, t in zip(dists, times)]

        lw_dist, rw_dist = 0, 0
        for i in range(len(g) - 1):
            for wrist in ['left_wrist', 'right_wrist']:
                x1, y1 = g.iloc[i][f'{wrist}_x'], g.iloc[i][f'{wrist}_y']
                x2, y2 = g.iloc[i+1][f'{wrist}_x'], g.iloc[i+1][f'{wrist}_y']
                if pd.notna(x1) and pd.notna(x2):
                    dist = np.linalg.norm([x2 - x1, y2 - y1])
                    if wrist == 'left_wrist': lw_dist += dist
                    else: rw_dist += dist

        metrics.append({
            'track_id': tid, 'step_id': sid,
            'activity': g['activity'].mode().iloc[0] if not g['activity'].mode().empty else 'Unknown',
            'duration': g['timestamp'].max() - g['timestamp'].min(),
            'total_distance': np.sum(dists),
            'avg_speed': np.mean(speeds),
            'max_speed': np.max(speeds),
            'left_wrist_distance': lw_dist,
            'right_wrist_distance': rw_dist,
            'total_hand_movement': lw_dist + rw_dist
        })

    df = pd.DataFrame(metrics)
    if not df.empty:
        df.to_csv(os.path.join(output_dir, 'pose_motion_metrics.csv'), index=False)
    return df

def run_pose_motion_analysis(base_path, pose_model, frame_interval=30):
    results_dir = os.path.join(base_path, "results")
    video_dirs = sorted(os.listdir(results_dir))

    for video_id in tqdm(video_dirs, desc="Batch Pose Motion Analysis"):
        video_result_path = os.path.join(results_dir, video_id)
        frames_path = os.path.join(video_result_path, "preprocessed_frames.csv")
        if not os.path.exists(frames_path):
            print(f"âŒ Skipping {video_id} â€“ no preprocessed frames")
            continue

        try:
            frames_df = pd.read_csv(frames_path)
            tracking_out = os.path.join(video_result_path, "pose_tracking")
            metrics_out = os.path.join(video_result_path, "pose_motion_metrics")

            tracks_df = track_poses(pose_model, frames_df, tracking_out, video_id=video_id)
            motion_df = calculate_pose_motion_metrics(tracks_df, metrics_out)
            print(f"âœ… Done: {video_id} â€” {len(tracks_df)} poses, {len(motion_df)} motion entries")

        except Exception as e:
            print(f"âš ï¸ Error processing {video_id}: {e}")

# -------------------- MAIN --------------------

BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset'

print("ðŸ§  Loading YOLOv8 pose model...")
pose_model = YOLO("yolov8m-pose.pt")
if torch.cuda.is_available():
    pose_model.to("cuda")
    print("ðŸš€ Model on GPU")
else:
    pose_model.to("cpu")
    print("âš ï¸ Model running on CPU")

print("ðŸƒ Running batch pose motion analysis...")
run_pose_motion_analysis(BASE_PATH, pose_model)
print("âœ… All videos processed.")

"""## 5. Muda Classifier

This code implements a comprehensive system for detecting all seven types of Lean waste (Muda): Rule-Based Detection Framework: Defines specific detection parameters for each waste type Specialized Detection Functions: Transportation waste: Detects unnecessary movement of materials across distances Inventory waste: Identifies excess materials waiting to be processed Motion waste: Detects unnecessary movement of workers/hands Waiting waste: Identifies idle time between process steps Overproduction waste: Detects producing more than required Overprocessing waste: Identifies excessive work on components Defects waste: Detects rework and corrections Waste Analysis and Visualization: Summary of waste by activity and type Timeline visualization of waste occurrences Confidence scores for each detection For your master thesis, this component directly addresses your research questions about what types of inefficiencies can be detected through automated video analysis (RQ1) and how effectively the system can detect and classify Lean waste types (RQ4). Once you've run this code successfully in your Colab notebook, we'll move on to Step 6: LLM Integration, which will generate explanations and improvement suggestions for each detected waste instance.

Transportation Waste
Definition: Unnecessary movement of materials or products between processes Detection Rules: Track object movements across frames using motion tracking data Calculate total distance traveled by materials/components Flag as waste when: Materials move more than X pixels (threshold adjustable based on workspace size) Objects are moved multiple times between the same locations Materials are transported without being processed between movements Example: When a wooden piece is moved across the workbench multiple times without value-adding work being performed on it

Inventory Waste
Definition: Excess materials or components waiting to be processed Detection Rules: Monitor the presence and usage of materials in the frame Calculate how long materials remain visible but unused Flag as waste when: Materials are present for extended periods without being used (>X seconds) Multiple instances of the same material type are visible but only one is being used Materials are arranged in a way that creates clutter (detected by proximity analysis) Example: When multiple screws are laid out but remain unused for a long period during the assembly process

Motion Waste
Definition: Unnecessary movement of workers Detection Rules: Analyze human pose tracking data from YOLO pose model Calculate hand/arm movement distances and patterns Flag as waste when: Repetitive movements occur without adding value Excessive reaching or stretching is detected (distance between body center and hands exceeds threshold) Worker makes unnecessary movements to access tools or materials Hand movements follow inefficient paths Example: When the person repeatedly reaches across the workbench for tools instead of having them arranged efficiently

Waiting Waste
Definition: Idle time when no value-adding work is being performed Detection Rules: Detect periods of inactivity in the process Analyze the time gaps between active operations Flag as waste when: No significant object or human movement is detected for >X seconds Tools or materials are stationary during a process step Transitions between activities take longer than expected Example: When there's a delay between finishing drilling and starting sanding because tools aren't readily available

Overproduction Waste
Definition: Producing more than is required or before it is needed Detection Rules: Count the number of similar components produced Compare with expected quantities for the process Flag as waste when: More components of the same type are produced than needed for the box Components are produced well before they're needed in the assembly sequence Same operation is performed on multiple identical pieces when only one is required Example: When multiple wooden sides are prepared but only some are used in the final assembly

Overprocessing Waste
Definition: Doing more work than necessary or adding features that aren't required Detection Rules: Monitor the time spent on each processing activity Analyze the number of tool interactions with each component Flag as waste when: Same tool is used on the same component multiple times beyond necessary Processing time for a component significantly exceeds the average time Multiple different tools are used for a task that could be completed with fewer tools Example: When excessive sanding is performed beyond what's needed for the box's functional requirements

Defects Waste
Definition: Errors requiring rework or correction Detection Rules: Detect when a completed step is repeated Identify when disassembly occurs after assembly Monitor for correction activities Flag as waste when: A component is worked on, then set aside and replaced with another Assembly steps are followed by immediate disassembly Same operation is performed twice on the same component The process flow reverses to a previous step Example: When a piece is assembled, then removed and reworked because it doesn't fit properly Each of these rules has configurable thresholds that can be adjusted based on the specific workspace and process requirements. The system calculates a confidence score for each waste detection based on how strongly the observed patterns match these rules. For your thesis, this rule-based approach provides a transparent and explainable framework for waste detection that directly incorporates Lean manufacturing principles, making it ideal for academic analysis and demonstration.

### Motion Waste Rule

ðŸŽ¯ Objective:
To detect Motion Wasteâ€”unnecessary or excessive movements by the workerâ€”by analyzing human pose keypoints over time using a rule-based and adaptive approach, aligned with Lean Management principles.

ðŸ” Input Files:
Each video is preprocessed to produce the following inputs per video:

pose_motion_metrics.csv: Summary metrics per (track_id, step_id) combination, including total_hand_movement, duration, etc.
pose_tracks.csv: Detailed pose keypoints for each detected person per frame.
ðŸ§  Detection Logic:
We detect motion waste using two main rules:

Rule 1: Excessive Motion

Measures total hand movement (total_hand_movement) over a process step.
Uses an adaptive threshold:
Threshold
=
Î¼
+
Î±
â‹…
Ïƒ
Threshold=Î¼+Î±â‹…Ïƒ
where:

Î¼
Î¼: mean hand movement across all tracked segments
Ïƒ
Ïƒ: standard deviation
Î±
Î±: sensitivity constant (default = 1.0)
If the movement exceeds the threshold, it is flagged as excessive.
Rule 2: Overreaching

Calculates the distance from wrist to hip in every frame.
Uses dynamic normalization based on torso length (shoulder-to-hip distance).
Overreaching is flagged when wrist distance > 1.5Ã— torso length in more than a threshold ratio of frames.
The overreach ratio threshold is also adaptive:
Threshold
=
meanÂ ratio
+
Î±
â‹…
std
Threshold=meanÂ ratio+Î±â‹…std
âš–ï¸ Final Classification:
For each (track_id, step_id):

is_motion_waste = True if either rule is triggered.
A confidence score is calculated:
\text{confidence} = 0.5 \cdot \text{excessive_motion} + 0.5 \cdot \text{excessive_reach}
Each detection includes:

step_id, track_id, activity, duration
Flags for excessive_motion, excessive_reach
overreach_ratio, confidence, is_motion_waste
ðŸ§ª Execution (Batch Processing):
The classifier is executed on all videos in the dataset:

For each video:
Loads the corresponding metrics and pose data
Applies the motion waste classifier
Saves the results to motion_waste.csv inside the waste_detection/ folder
ðŸ“Š Visualization:
For each video, we generate a bar plot showing the number of motion waste occurrences per step:

X-axis: step_id
Y-axis: motion waste count
Output saved as: {video_id}_motion_waste_per_step.png
âœ… Benefits of This Approach:
Transparent & explainable rules aligned with Lean theory
Video-specific adaptiveness to account for different processes, workers, and speeds
Reusable for visual validation and debugging
Ready for integration with further waste types and LLM explanations
"""

import numpy as np
import pandas as pd

def detect_motion_waste(motion_df, pose_tracks_df, alpha_motion=1.0, alpha_reach=1.0):
    # --- Compute global torso length for stability ---
    torso_lengths = []
    for _, r in pose_tracks_df.iterrows():
        if pd.notna(r['left_shoulder_x']) and pd.notna(r['left_hip_x']):
            torso = np.linalg.norm([
                r['left_shoulder_x'] - r['left_hip_x'],
                r['left_shoulder_y'] - r['left_hip_y']
            ])
            if torso > 0:
                torso_lengths.append(torso)
    global_torso = np.mean(torso_lengths) if torso_lengths else 1.0

    # --- Compute movement threshold ---
    movement_threshold = motion_df['total_hand_movement'].mean() + alpha_motion * motion_df['total_hand_movement'].std()

    # --- Compute overreach ratio per person-step ---
    all_ratios = []
    for (track_id, step_id), person_pose in pose_tracks_df.groupby(['track_id', 'step_id']):
        if len(person_pose) < 2:
            continue
        overreach_count = 0
        total_frames = 0
        for _, r in person_pose.iterrows():
            for wrist in ['left_wrist', 'right_wrist']:
                if pd.notna(r[f'{wrist}_x']) and pd.notna(r['left_hip_x']):
                    dist = np.linalg.norm([
                        r[f'{wrist}_x'] - r['left_hip_x'],
                        r[f'{wrist}_y'] - r['left_hip_y']
                    ])
                    if dist > global_torso * 1.5:
                        overreach_count += 1
            total_frames += 1
        if total_frames > 0:
            all_ratios.append(overreach_count / total_frames)

    ratio_threshold = np.mean(all_ratios) + alpha_reach * np.std(all_ratios) if all_ratios else 0.4

    # --- Detect motion waste per step ---
    waste_flags = []
    for _, row in motion_df.iterrows():
        track_id = row['track_id']
        step_id = row['step_id']
        total_movement = row['total_hand_movement']
        excessive_motion = total_movement > movement_threshold

        person_pose = pose_tracks_df[
            (pose_tracks_df['track_id'] == track_id) &
            (pose_tracks_df['step_id'] == step_id)
        ]
        overreach_count = 0
        total_frames = 0
        for _, r in person_pose.iterrows():
            for wrist in ['left_wrist', 'right_wrist']:
                if pd.notna(r[f'{wrist}_x']) and pd.notna(r['left_hip_x']):
                    dist = np.linalg.norm([
                        r[f'{wrist}_x'] - r['left_hip_x'],
                        r[f'{wrist}_y'] - r['left_hip_y']
                    ])
                    if dist > global_torso * 1.5:
                        overreach_count += 1
            total_frames += 1

        overreach_ratio = overreach_count / total_frames if total_frames > 0 else 0
        excessive_reach = overreach_ratio > ratio_threshold
        is_motion_waste = excessive_motion or excessive_reach
        confidence = 0.5 * float(excessive_motion) + 0.5 * float(excessive_reach)

        explanation_parts = []
        if excessive_motion:
            explanation_parts.append("high hand movement")
        if excessive_reach:
            explanation_parts.append("frequent overreaching")
        explanation = "Motion waste due to " + " and ".join(explanation_parts) if explanation_parts else "No motion waste detected."

        waste_flags.append({
            'step_id': step_id,
            'track_id': track_id,
            'activity': row['activity'],
            'duration': row['duration'],
            'total_hand_movement': total_movement,
            'movement_threshold': movement_threshold,
            'overreach_ratio': overreach_ratio,
            'ratio_threshold': ratio_threshold,
            'excessive_motion': excessive_motion,
            'excessive_reach': excessive_reach,
            'confidence': confidence,
            'is_motion_waste': is_motion_waste,
            'explanation': explanation
        })

    return pd.DataFrame(waste_flags)

import os

def batch_detect_motion_waste(BASE_PATH, alpha_motion=1.0, alpha_reach=1.0):
    results_dir = os.path.join(BASE_PATH, "results")
    video_dirs = sorted(os.listdir(results_dir))

    for video_id in video_dirs:
        print(f"ðŸ” Processing {video_id}...")

        motion_path = os.path.join(results_dir, video_id, "pose_motion_metrics", "pose_motion_metrics.csv")
        pose_path = os.path.join(results_dir, video_id, "pose_tracking", "pose_tracks.csv")

        if not os.path.exists(motion_path) or not os.path.exists(pose_path):
            print(f"âš ï¸ Skipping {video_id} (missing files)")
            continue

        motion_df = pd.read_csv(motion_path)
        pose_tracks_df = pd.read_csv(pose_path)

        motion_waste_df = detect_motion_waste(motion_df, pose_tracks_df, alpha_motion=alpha_motion, alpha_reach=alpha_reach)

        output_path = os.path.join(results_dir, video_id, "waste_detection")
        os.makedirs(output_path, exist_ok=True)
        motion_waste_df.to_csv(os.path.join(output_path, "motion_waste.csv"), index=False)

        # Optional per-step visualization
        plot_motion_waste_per_step(motion_waste_df, video_id, output_path)

import matplotlib.pyplot as plt
import seaborn as sns

def plot_motion_waste_per_step(motion_waste_df, video_id, output_path):
    sns.set(style="whitegrid")

    step_summary = (
        motion_waste_df[motion_waste_df['is_motion_waste']]
        .groupby('step_id')
        .size()
        .reset_index(name='motion_waste_count')
        .sort_values('motion_waste_count', ascending=False)
    )

    plt.figure(figsize=(12, 6))
    sns.barplot(data=step_summary, x='step_id', y='motion_waste_count', palette='mako')
    plt.title(f'Motion Waste per Step: {video_id}')
    plt.xlabel('Step ID')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(os.path.join(output_path, f'{video_id}_motion_waste_per_step.png'))
    plt.close()

BASE_PATH = "/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset"
batch_detect_motion_waste(BASE_PATH)

import pandas as pd
import os
import matplotlib.pyplot as plt

def plot_motion_waste_summary(BASE_PATH):
    summary = []

    results_dir = os.path.join(BASE_PATH, "results")
    for video_id in sorted(os.listdir(results_dir)):
        waste_path = os.path.join(results_dir, video_id, "waste_detection", "motion_waste.csv")
        if not os.path.exists(waste_path):
            continue
        df = pd.read_csv(waste_path)
        count = df['is_motion_waste'].sum()
        summary.append({'video_id': video_id, 'motion_waste_count': count})

    summary_df = pd.DataFrame(summary)
    summary_df = summary_df.sort_values('motion_waste_count', ascending=False)

    # ðŸ“Š Plot
    plt.figure(figsize=(14, 6))
    plt.bar(summary_df['video_id'], summary_df['motion_waste_count'], color='slateblue')
    plt.xticks(rotation=45, ha='right')
    plt.xlabel("Video")
    plt.ylabel("Motion Waste Count")
    plt.title("Motion Waste per Video")
    plt.tight_layout()
    plt.show()

plot_motion_waste_summary(BASE_PATH)

import os
import pandas as pd

def create_motion_waste_all_file(BASE_PATH, results_folder='results'):
    all_motion_data = []

    results_dir = os.path.join(BASE_PATH, results_folder)
    if not os.path.exists(results_dir):
        print(f"âŒ Folder not found: {results_dir}")
        return None, None

    for video_id in sorted(os.listdir(results_dir)):
        motion_file = os.path.join(results_dir, video_id, "waste_detection", "motion_waste.csv")
        if not os.path.exists(motion_file):
            continue
        df = pd.read_csv(motion_file)
        df['video_id'] = video_id
        all_motion_data.append(df)

    if all_motion_data:
        combined_df = pd.concat(all_motion_data, ignore_index=True)
        output_path = os.path.join(BASE_PATH, "motion_waste_all.csv")
        combined_df.to_csv(output_path, index=False)
        print(f"âœ… Saved: {output_path}")
        return combined_df, output_path
    else:
        print("âš ï¸ No motion_waste.csv files found.")
        return None, None

# Set your base path
BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset'

# Run it
motion_waste_all_df, path = create_motion_waste_all_file(BASE_PATH)

"""### Waiting Rule"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def detect_waiting_waste_across_dataset(results_path, alpha=1.0):
    all_results = []

    for video_id in sorted(os.listdir(results_path)):
        video_path = os.path.join(results_path, video_id)
        motion_file = os.path.join(video_path, 'pose_motion_metrics', 'pose_motion_metrics.csv')

        if not os.path.exists(motion_file):
            print(f"âš ï¸ Skipping {video_id} â€” no pose motion metrics found.")
            continue

        motion_df = pd.read_csv(motion_file)
        if motion_df.empty:
            continue

        # --------- Adaptive Thresholds ---------
        min_idle_time = motion_df['duration'].mean() + alpha * motion_df['duration'].std()
        max_motion_during_wait = motion_df['total_distance'].quantile(0.25)
        person_idle_threshold = motion_df['avg_speed'].quantile(0.25)

        step_groups = motion_df.groupby('step_id')

        for step_id, step_data in step_groups:
            duration = step_data['duration'].mean()
            total_distance = step_data['total_distance'].mean()
            avg_speed = step_data['avg_speed'].mean()
            activity = step_data['activity'].mode().iloc[0] if not step_data['activity'].mode().empty else 'Unknown'

            is_waiting = (
                duration > min_idle_time and
                total_distance < max_motion_during_wait and
                avg_speed < person_idle_threshold
            )

            # --- Scaled & Clipped Confidence ---
            duration_score = np.clip((duration - min_idle_time) / min_idle_time, 0, 1)
            distance_score = np.clip(1 - (total_distance / max_motion_during_wait), 0, 1)
            speed_score = np.clip(1 - (avg_speed / person_idle_threshold), 0, 1)
            confidence = 0.4 * duration_score + 0.3 * distance_score + 0.3 * speed_score

            explanation = f"Step lasted {duration:.2f}s with low speed ({avg_speed:.3f}) and movement ({total_distance:.3f})."

            all_results.append({
                'video_id': video_id,
                'step_id': step_id,
                'activity': activity,
                'duration': duration,
                'avg_speed': avg_speed,
                'total_distance': total_distance,
                'is_waiting_waste': is_waiting,
                'confidence': round(confidence, 3),
                'explanation': explanation,
                'min_idle_time': round(min_idle_time, 2),
                'max_motion_threshold': round(max_motion_during_wait, 3),
                'speed_threshold': round(person_idle_threshold, 3)
            })

    return pd.DataFrame(all_results)

def visualize_waiting_waste(waiting_df):
    sns.set(style="whitegrid")
    plt.figure(figsize=(12, 6))
    sns.barplot(
        data=waiting_df[waiting_df['is_waiting_waste']],
        x='step_id', y='confidence', hue='video_id', dodge=False, palette='crest')
    plt.title("â³ Waiting Waste Confidence per Step")
    plt.xlabel("Step ID")
    plt.ylabel("Confidence Score")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

RESULTS_PATH = '/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/results'

# Run detection
waiting_df = detect_waiting_waste_across_dataset(RESULTS_PATH)

# Visualize
visualize_waiting_waste(waiting_df)

# Save to CSV
output_file = os.path.join(RESULTS_PATH, "waiting_waste_all.csv")
waiting_df.to_csv(output_file, index=False)
print(f"âœ… Waiting waste saved to {output_file} â€” total flagged = {waiting_df['is_waiting_waste'].sum()}")

print("Videos processed:", waiting_df['video_id'].nunique())
print("Total steps analyzed:", len(waiting_df))
print("Steps flagged as waiting waste:", waiting_df['is_waiting_waste'].sum())

"""### Inventory Rule"""

import os
import pandas as pd
import numpy as np

# Example production requirements
def load_production_requirements():
    return {
        '1': {  # measuring
            'Ruler': 1,
            'Pencil': 1,
            'Small Wood': 2
        },
        '2': {  # drilling
            'Drill': 1,
            'Small Wood': 2
        },
        '3': {  # sanding
            'Sand Block': 1,
            'Small Wood': 2
        },
        '4': {  # assembling
            'Screwdriver': 1,
            'Screw': 4,
            'Big Wood': 2,
            'Glue': 1
        },
        '5': {  # screwing the top
            'Screwdriver': 1,
            'Screw': 4,
            'Tape': 1
        },
        '6': {  # varnishing
            'Tape Roll': 1,
            'Wooden Box': 1
        },
        '7': {  # drying
            'Wooden Box': 1
        },
        '8': {  # unscrewing the top
            'Screwdriver': 1,
            'Screw': 4
        },
        '9': {  # disassembling
            'Hammer': 1,
            'Small Wood': 2,
            'Big Wood': 2
        }
    }

# Analyze if objects arrive too early, stay too long, or idle
def analyze_temporal_waste(step_data, item_class):
    item_data = step_data[step_data['class_name'] == item_class]
    if item_data.empty:
        return {'early_arrival_time': 0, 'late_departure_time': 0, 'inactive_presence': 0, 'has_waste': False}

    step_start = step_data['timestamp'].min()
    step_end = step_data['timestamp'].max()
    item_first_seen = item_data['timestamp'].min()
    item_last_seen = item_data['timestamp'].max()

    early = max(0, step_start - item_first_seen)
    late = max(0, item_last_seen - step_end)
    total_time = step_end - step_start if step_end > step_start else 1
    inactive_presence = ((item_last_seen - item_first_seen) - total_time) / total_time
    has_waste = early > 30 or late > 30 or inactive_presence > 0.2

    return {
        'early_arrival_time': early,
        'late_departure_time': late,
        'inactive_presence': inactive_presence,
        'has_waste': has_waste
    }

# Confidence calculation based on multiple indicators
def calculate_improved_confidence(excess_qty, unnecessary_items, temporal_waste, required_qty):
    confidence_factors = []

    if required_qty > 0:
        confidence_factors.append(min(excess_qty / required_qty, 1.0))
    if unnecessary_items > 0:
        confidence_factors.append(1.0)
    if temporal_waste['early_arrival_time'] > 0:
        confidence_factors.append(min(temporal_waste['early_arrival_time'] / 300, 1.0))
    if temporal_waste['late_departure_time'] > 0:
        confidence_factors.append(min(temporal_waste['late_departure_time'] / 300, 1.0))
    if confidence_factors:
        return round(np.mean(confidence_factors), 3)
    else:
        return 0.0

# Main function to detect inventory waste against production expectations
def detect_inventory_waste_vs_requirements(results_path, requirements=None):
    if requirements is None:
        requirements = load_production_requirements()

    all_results = []

    for video_id in sorted(os.listdir(results_path)):
        object_file = os.path.join(results_path, video_id, 'object_tracking', 'tracked_objects.csv')
        if not os.path.exists(object_file):
            print(f"âš ï¸ Skipping {video_id} â€” no object tracking file.")
            continue

        df = pd.read_csv(object_file)
        if df.empty or 'step_id' not in df.columns or 'timestamp' not in df.columns:
            print(f"âš ï¸ Skipping {video_id} â€” missing step_id or timestamp.")
            continue

        for step_id in df['step_id'].unique():
            step_data = df[df['step_id'] == step_id]
            step_req = requirements.get(str(step_id), {})

            for class_name in step_data['class_name'].unique():
                obj_data = step_data[step_data['class_name'] == class_name]
                actual_qty = obj_data['track_id'].nunique()
                required_qty = step_req.get(class_name, 0)

                excess_qty = max(0, actual_qty - required_qty)
                unnecessary_items = actual_qty if required_qty == 0 else 0
                temporal = analyze_temporal_waste(step_data, class_name)
                is_waste = excess_qty > 0 or unnecessary_items > 0 or temporal['has_waste']
                confidence = calculate_improved_confidence(excess_qty, unnecessary_items, temporal, required_qty)

                all_results.append({
                    'video_id': video_id,
                    'step_id': step_id,
                    'class_name': class_name,
                    'actual_quantity': actual_qty,
                    'required_quantity': required_qty,
                    'excess_quantity': excess_qty,
                    'unnecessary_items': unnecessary_items,
                    'temporal_waste': temporal['has_waste'],
                    'confidence': confidence,
                    'is_inventory_waste': is_waste,
                    'early_arrival_time': temporal['early_arrival_time'],
                    'late_departure_time': temporal['late_departure_time'],
                    'inactive_presence': round(temporal['inactive_presence'], 3)
                })

    return pd.DataFrame(all_results)

RESULTS_PATH = '/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/results'

# Run detection
inventory_vs_req_df = detect_inventory_waste_vs_requirements(RESULTS_PATH)

# Save result
inventory_vs_req_df.to_csv(os.path.join(RESULTS_PATH, 'inventory_waste_vs_requirements.csv'), index=False)

import matplotlib.pyplot as plt
import seaborn as sns

def plot_inventory_waste_counts(df):
    sns.set(style="whitegrid")
    plt.figure(figsize=(14, 6))

    # Count number of waste detections per step and object class
    count_df = (
        df[df['is_inventory_waste']]
        .groupby(['step_id', 'class_name'])
        .size()
        .reset_index(name='waste_count')
    )

    sns.barplot(
        data=count_df,
        x='step_id', y='waste_count',
        hue='class_name', palette='flare', dodge=True
    )

    plt.title("Number of Inventory Waste Events per Step")
    plt.xlabel("Step ID")
    plt.ylabel("Waste Count")
    plt.xticks(rotation=45)
    plt.legend(title='Object Class', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

plot_inventory_waste_counts(inventory_vs_req_df)

"""### Transportation rule"""

import os
import pandas as pd
import numpy as np

def compute_transportation_distance(results_path):
    print("ðŸ“¦ Updating tracked_objects.csv files with total_distance...")

    for video_id in sorted(os.listdir(results_path)):
        obj_path = os.path.join(results_path, video_id, "object_tracking", "tracked_objects.csv")
        if not os.path.exists(obj_path):
            print(f"âš ï¸ Skipping {video_id} â€” tracked_objects.csv not found.")
            continue

        df = pd.read_csv(obj_path)

        required_cols = {'track_id', 'x1', 'y1', 'x2', 'y2'}
        if not required_cols.issubset(df.columns):
            print(f"âš ï¸ Skipping {video_id} â€” missing required columns.")
            continue

        df['x_center'] = (df['x1'] + df['x2']) / 2
        df['y_center'] = (df['y1'] + df['y2']) / 2

        total_distances = []
        for track_id, group in df.groupby('track_id'):
            group = group.sort_values('frame_num')
            dx = group['x_center'].diff().fillna(0)
            dy = group['y_center'].diff().fillna(0)
            distance = np.sqrt(dx**2 + dy**2).sum()
            total_distances.append((track_id, distance))

        # Map total distance back to dataframe
        distance_map = dict(total_distances)
        df['total_distance'] = df['track_id'].map(distance_map)

        # Save back
        df.to_csv(obj_path, index=False)
        print(f"âœ… {video_id} updated with total_distance.")

# Run once before detection
# BASE_PATH is your dataset base path
BASE_PATH = "/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/results"
compute_transportation_distance(BASE_PATH)

def detect_transportation_waste(results_path, alpha=1.0):
    all_transport = []

    for video_id in sorted(os.listdir(results_path)):
        file_path = os.path.join(results_path, video_id, "object_tracking", "tracked_objects.csv")
        if not os.path.exists(file_path):
            continue

        df = pd.read_csv(file_path)
        if 'total_distance' not in df.columns or df.empty:
            continue

        # Only consider non-person objects
        df = df[df['class_name'] != 'Person']

        if df.empty:
            continue

        # Adaptive threshold
        movement_threshold = df['total_distance'].mean() + alpha * df['total_distance'].std()

        for (step_id, track_id), group in df.groupby(['step_id', 'track_id']):
            total_distance = group['total_distance'].iloc[0]
            class_name = group['class_name'].iloc[0]
            activity = group['activity'].mode().iloc[0] if not group['activity'].mode().empty else 'Unknown'

            is_waste = total_distance > movement_threshold
            confidence = min((total_distance - movement_threshold) / movement_threshold, 1.0) if is_waste else 0

            all_transport.append({
                'video_id': video_id,
                'step_id': step_id,
                'track_id': track_id,
                'object_class': class_name,
                'activity': activity,
                'total_distance': total_distance,
                'is_transportation_waste': is_waste,
                'confidence': confidence
            })

    return pd.DataFrame(all_transport)

# Run detection
transport_df = detect_transportation_waste(BASE_PATH)
transport_df.to_csv(os.path.join(BASE_PATH, "transportation_waste_all.csv"), index=False)
print(f"âœ… Transportation waste saved. Total flagged = {transport_df['is_transportation_waste'].sum()}")

import seaborn as sns
import matplotlib.pyplot as plt

def plot_transportation_waste_per_step(df):
    plt.figure(figsize=(12, 6))
    sns.countplot(data=df[df['is_transportation_waste']], x='step_id', palette='flare')
    plt.title("ðŸš› Transportation Waste Instances per Step")
    plt.xlabel("Step ID")
    plt.ylabel("Number of Wasteful Movements")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Load and plot
transport_df = pd.read_csv(f"{BASE_PATH}/transportation_waste_all.csv")
plot_transportation_waste_per_step(transport_df)

def plot_top_wasteful_objects(df, top_n=10):
    plt.figure(figsize=(10, 5))
    sns.countplot(
        data=df[df['is_transportation_waste']],
        y='object_class', order=df[df['is_transportation_waste']]['object_class'].value_counts().index[:top_n],
        palette='magma'
    )
    plt.title("ðŸ“¦ Top Wasteful Objects by Transportation")
    plt.xlabel("Count")
    plt.ylabel("Object Class")
    plt.tight_layout()
    plt.show()

plot_top_wasteful_objects(transport_df)

def plot_transportation_confidence(df):
    agg = df[df['is_transportation_waste']].groupby('step_id')['confidence'].mean().reset_index()

    plt.figure(figsize=(12, 5))
    sns.barplot(data=agg, x='step_id', y='confidence', palette='coolwarm')
    plt.title("ðŸŽ¯ Avg Transportation Waste Confidence per Step")
    plt.xlabel("Step ID")
    plt.ylabel("Avg Confidence")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_transportation_confidence(transport_df)

"""### Overproduction Rule"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def detect_overproduction_waste_across_dataset(results_path, alpha=1.0):
    overproduction_flags = []

    for video_id in sorted(os.listdir(results_path)):
        video_path = os.path.join(results_path, video_id)
        object_file = os.path.join(video_path, 'object_tracking', 'tracked_objects.csv')
        steps_file = os.path.join(video_path, 'preprocessed_steps.csv')

        if not os.path.exists(object_file) or not os.path.exists(steps_file):
            print(f"âš ï¸ Skipping {video_id} â€” missing tracked_objects.csv or preprocessed_steps.csv")
            continue

        try:
            obj_df = pd.read_csv(object_file)
            steps_df = pd.read_csv(steps_file)
        except Exception as e:
            print(f"âš ï¸ Error reading files for {video_id}: {e}")
            continue

        if obj_df.empty or 'step_id' not in obj_df.columns or 'class_name' not in obj_df.columns:
            continue

        tool_classes = ['Screwdriver', 'Hammer', 'Drill', 'Glue']
        obj_df = obj_df[obj_df['class_name'].isin(tool_classes)]

        if obj_df.empty:
            continue

        # Count tool switches per step
        switch_counts = obj_df.groupby('step_id')['class_name'].nunique()

        # Estimate duration from timestamp range per step
        if 'timestamp' in obj_df.columns:
            duration_by_step = obj_df.groupby('step_id')['timestamp'].agg(lambda x: x.max() - x.min())
        else:
            duration_by_step = pd.Series(index=switch_counts.index, data=0)

        switch_mean = switch_counts.mean()
        switch_std = switch_counts.std()
        switch_threshold = switch_mean + alpha * switch_std

        for step_id, count in switch_counts.items():
            switches = count
            duration = duration_by_step.get(step_id, 0)
            activity = steps_df[steps_df['step_id'] == step_id]['activity'].mode().iloc[0] if not steps_df[steps_df['step_id'] == step_id].empty else "Unknown"

            is_overproduction = switches > switch_threshold
            confidence = min(1.0, (switches - switch_threshold) / switch_threshold) if is_overproduction else 0.0

            overproduction_flags.append({
                'video_id': video_id,
                'step_id': step_id,
                'activity': activity,
                'estimated_duration': duration,
                'tool_switches': switches,
                'is_overproduction_waste': is_overproduction,
                'confidence': confidence
            })

    return pd.DataFrame(overproduction_flags)

# ------------------- Visualization -------------------

def visualize_overproduction(over_df):
    if 'is_overproduction_waste' not in over_df.columns:
        print("âŒ Column 'is_overproduction_waste' not found.")
        return

    sns.set(style="whitegrid")
    plt.figure(figsize=(12, 6))
    sns.barplot(
        data=over_df[over_df['is_overproduction_waste']],
        x='step_id', y='confidence', hue='video_id', dodge=False, palette='flare'
    )
    plt.title("ðŸ” Overproduction Waste per Step")
    plt.xlabel("Step ID")
    plt.ylabel("Confidence Score")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# ------------------- Run + Save -------------------

results_path = "/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/results"

over_df = detect_overproduction_waste_across_dataset(results_path)
visualize_overproduction(over_df)

# Save CSV
output_path = os.path.join(results_path, "overproduction_waste_all.csv")
over_df.to_csv(output_path, index=False)
print(f"âœ… Saved: {output_path}")
print(f"ðŸŸ¢ Total overproduction waste flags: {over_df['is_overproduction_waste'].sum()}")

"""### Overproccesing Rule"""

import os
import pandas as pd

def patch_step_files_with_duration(results_path):
    for video_id in sorted(os.listdir(results_path)):
        video_path = os.path.join(results_path, video_id)
        step_file = os.path.join(video_path, "preprocessed_steps.csv")

        if not os.path.exists(step_file):
            continue

        df = pd.read_csv(step_file)

        if 'start_time' in df.columns and 'end_time' in df.columns and 'duration' not in df.columns:
            df['duration'] = df['end_time'] - df['start_time']
            df.to_csv(step_file, index=False)
            print(f"âœ… Added duration to: {video_id}")
        else:
            print(f"âš ï¸ Skipping {video_id} â€” already patched or missing time columns.")

# Run this once before detection
results_path = "/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/results"
patch_step_files_with_duration(results_path)

import numpy as np

def detect_overprocessing_waste_across_dataset(results_path, alpha=1.0):
    all_flags = []

    for video_id in sorted(os.listdir(results_path)):
        video_path = os.path.join(results_path, video_id)
        step_file = os.path.join(video_path, "preprocessed_steps.csv")

        if not os.path.exists(step_file):
            continue

        df = pd.read_csv(step_file)

        # Check required columns
        required_cols = ['step_id', 'activity', 'duration']
        if not all(col in df.columns for col in required_cols):
            print(f"âš ï¸ Skipping {video_id} â€” missing required columns.")
            continue

        if df.empty:
            continue

        # Adaptive threshold
        duration_threshold = df['duration'].mean() + alpha * df['duration'].std()

        for _, row in df.iterrows():
            is_over = row['duration'] > duration_threshold
            confidence = (row['duration'] - duration_threshold) / duration_threshold
            confidence = np.clip(confidence, 0, 1)

            all_flags.append({
                'video_id': video_id,
                'step_id': row['step_id'],
                'activity': row['activity'],
                'duration': row['duration'],
                'is_overprocessing_waste': is_over,
                'confidence': confidence
            })

    return pd.DataFrame(all_flags)

import seaborn as sns
import matplotlib.pyplot as plt

def visualize_overprocessing(df):
    if 'is_overprocessing_waste' not in df.columns:
        print("âš ï¸ Column 'is_overprocessing_waste' not found.")
        return

    plot_df = df[df['is_overprocessing_waste']]
    if plot_df.empty:
        print("â„¹ï¸ No overprocessing waste detected.")
        return

    sns.set(style="whitegrid")
    plt.figure(figsize=(12, 6))
    sns.barplot(
        data=plot_df,
        x='step_id', y='confidence', hue='video_id', dodge=False, palette='flare'
    )
    plt.title("ðŸ”§ Overprocessing Waste per Step")
    plt.xlabel("Step ID")
    plt.ylabel("Confidence Score")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

results_path = "/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/results"

# Step 1: Ensure all step files have 'duration'
patch_step_files_with_duration(results_path)

# Step 2: Run overprocessing waste detection
over_df = detect_overprocessing_waste_across_dataset(results_path)

# Step 3: Visualize
visualize_overprocessing(over_df)

# Step 4: Save to CSV
over_df.to_csv(os.path.join(results_path, "overprocessing_waste_all.csv"), index=False)
print("âœ… Saved: overprocessing_waste_all.csv")

"""### Defects Rule"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------- DEFECT WASTE DETECTION --------------------

def detect_defect_waste_across_dataset(results_path, alpha=1.0):
    all_defect_flags = []

    for video_id in sorted(os.listdir(results_path)):
        video_path = os.path.join(results_path, video_id)
        steps_file = os.path.join(video_path, "preprocessed_steps.csv")
        objects_file = os.path.join(video_path, "object_tracking", "tracked_objects.csv")

        if not os.path.exists(steps_file) or not os.path.exists(objects_file):
            print(f"âš ï¸ Skipping {video_id} â€” required files missing.")
            continue

        df_steps = pd.read_csv(steps_file)
        df_objs = pd.read_csv(objects_file)

        if df_steps.empty or 'step_id' not in df_steps.columns or 'activity' not in df_steps.columns or 'duration' not in df_steps.columns:
            print(f"âš ï¸ Skipping {video_id} â€” missing required step columns.")
            continue

        # --- Repetition + Long Duration Rule ---
        repeated_steps = (
            df_steps.groupby(['step_id', 'activity'])
            .size().reset_index(name='count')
        )

        duration_stats = df_steps.groupby('activity')['duration'].agg(['mean', 'std']).reset_index()
        df_steps = df_steps.merge(duration_stats, on='activity', how='left', suffixes=('', '_stat'))

        for _, row in df_steps.iterrows():
            step_id = row['step_id']
            activity = row['activity']
            duration = row['duration']
            mean_dur = row['mean']
            std_dur = row['std']
            threshold = mean_dur + alpha * std_dur if pd.notna(std_dur) else mean_dur

            repeats = repeated_steps[
                (repeated_steps['step_id'] == step_id) &
                (repeated_steps['activity'] == activity)
            ]
            repeat_count = int(repeats['count'].iloc[0]) if not repeats.empty else 1
            is_defect = duration > threshold and repeat_count > 1

            confidence = (
                ((duration - threshold) / threshold if threshold > 0 else 0) * 0.6 +
                (repeat_count - 1) * 0.4
            )
            confidence = np.clip(confidence, 0, 1)

            if is_defect:
                all_defect_flags.append({
                    'video_id': video_id,
                    'step_id': step_id,
                    'activity': activity,
                    'duration': duration,
                    'repeat_count': repeat_count,
                    'adaptive_duration_threshold': threshold,
                    'is_defect_waste': True,
                    'confidence': confidence,
                    'explanation': f"Activity '{activity}' repeated {repeat_count}x and lasted {duration:.2f}s, above adaptive threshold {threshold:.2f}s."
                })

        # --- Tool in Late Steps Rule (e.g. screwdriver) ---
        disassembly_tools = ['Screwdriver', 'Drill']
        mid_step = df_steps['step_id'].median() if not df_steps['step_id'].empty else 0
        late_steps = df_steps[df_steps['step_id'] > mid_step]['step_id'].unique().tolist()

        if 'class_name' in df_objs.columns and 'step_id' in df_objs.columns:
            disassembly_df = df_objs[
                df_objs['class_name'].isin(disassembly_tools) &
                df_objs['step_id'].isin(late_steps)
            ]

            for _, r in disassembly_df.iterrows():
                step_id = r['step_id']
                activity = r.get('activity', f'step_{step_id}')
                obj = r['class_name']
                track_id = r.get('track_id', -1)

                all_defect_flags.append({
                    'video_id': video_id,
                    'step_id': step_id,
                    'activity': activity,
                    'duration': None,
                    'repeat_count': None,
                    'adaptive_duration_threshold': None,
                    'is_defect_waste': True,
                    'confidence': 0.8,
                    'explanation': f"Detected use of {obj} in late step {step_id}, possibly disassembling/fixing."
                })

    return pd.DataFrame(all_defect_flags)

# -------------------- VISUALIZATION --------------------

def visualize_defect(defect_df):
    if defect_df.empty or 'is_defect_waste' not in defect_df.columns:
        print("âš ï¸ No defect waste data available for visualization.")
        return

    sns.set(style="whitegrid")
    plt.figure(figsize=(12, 6))
    sns.barplot(
        data=defect_df[defect_df['is_defect_waste']],
        x='step_id', y='confidence', hue='video_id', dodge=False, palette='flare')
    plt.title("ðŸ› ï¸ Defect Waste Confidence per Step")
    plt.xlabel("Step ID")
    plt.ylabel("Confidence Score")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# -------------------- EXECUTION --------------------

results_path = "/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/results"

defect_df = detect_defect_waste_across_dataset(results_path)
visualize_defect(defect_df)

# === SAVE RESULTS ===
defect_df.to_csv(
    os.path.join(results_path, "defect_waste_all.csv"),
    index=False
)
print(f"âœ… Defect waste detections saved. Total flagged = {defect_df['is_defect_waste'].sum()}")

"""### all togehter"""

import os
import pandas as pd

# Set root path
ROOT = "/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/results"

def combine_per_video_csvs(subfolder, filename, output_name):
    combined = []

    for video_id in sorted(os.listdir(ROOT)):
        video_path = os.path.join(ROOT, video_id)
        waste_file = os.path.join(video_path, subfolder, filename)

        if not os.path.exists(waste_file):
            print(f"âš ï¸ Skipping {video_id} â€” {filename} not found in {subfolder}")
            continue

        df = pd.read_csv(waste_file)
        df['video_id'] = video_id  # Track source
        combined.append(df)

    if not combined:
        print(f"âŒ No data found for {output_name}")
        return

    result_df = pd.concat(combined, ignore_index=True)
    save_path = os.path.join(ROOT, output_name)
    result_df.to_csv(save_path, index=False)
    print(f"âœ… Saved {output_name} â€” Total rows: {len(result_df)}")

# Combine both waste types
combine_per_video_csvs("waste_inventory", "inventory_waste.csv", "inventory_waste_all.csv")
combine_per_video_csvs("waste_detection", "motion_waste.csv", "motion_waste_all.csv")

import os
import pandas as pd

# Define path where all individual waste outputs are saved
waste_dir = "/content/drive/MyDrive/Colab Notebooks/Arbnor_MSc/MT/BOX_Dataset/MSc_Whole_Dataset/results"

# List of all waste CSV files you want to merge
waste_files = {
    'motion': "motion_waste_all.csv",
    'waiting': "waiting_waste_all.csv",
    'inventory': "inventory_waste_all.csv",
    'transportation': "transportation_waste_all.csv",
    'overproduction': "overproduction_waste_all.csv",
    'overprocessing': "overprocessing_waste_all.csv",
    'defect': "defect_waste_all.csv"
}

all_waste_df = []

for waste_type, file_name in waste_files.items():
    full_path = os.path.join(waste_dir, file_name)
    if os.path.exists(full_path):
        df = pd.read_csv(full_path)
        df['waste_type'] = waste_type  # Add waste type column
        all_waste_df.append(df)
        print(f"âœ… Loaded {waste_type} waste: {df.shape[0]} rows")
    else:
        print(f"âš ï¸ Missing: {file_name} â€” Skipped")

# Concatenate all into one
combined_df = pd.concat(all_waste_df, ignore_index=True)

# Save the combined output
combined_output_path = os.path.join(waste_dir, "combined_waste_detections.csv")
combined_df.to_csv(combined_output_path, index=False)

print(f"\nðŸ“¦ Combined waste file saved to: {combined_output_path}")
print(f"ðŸ§® Total combined detections: {combined_df.shape[0]}")